# 底层网络库设计与实现

---

### 传统处理IO

传统阻塞IO服务模式特点：

1. 采用阻塞I/O模式获取输入数据
2. 每个连接都需要独立的线程完成数据的输入，业务处理，数据返回

![image-20251117152831501](assets/image-20251117152831501.png)

问题分析：

1. 当并发数很大，就会创建大量线程，占用大量的系统资源
2. 连接创建后，如果当前线程暂时没有数据可读，该线程会阻塞在read操作，造成线程资源浪费、

### 事件驱动的Reactor模式

Reactor模式是处理并发IO比较常见的一种模式，Reactor模式基本设计思想就是I/O复用（poll、epoll、select） + 线程池

Reactor模式称为反应器模式或应答者模式，是基于事件驱动的设计模式，

1. Reactor模式有1个服务处理器和多个请求处理器，服务处理器会将输入的请求事件以多路复用的方式分发给相应的请求处理器。
    - 将所有要处理的IO事件注册到一个中心IO多路复用器上，同时<font color='#BAOC2F'>主线程or进程阻塞在多路复用器上</font>，
    - 一旦有IO事件到来或是准备就绪（文件描述符或socket可读可写），则多路复用器返回并将事先注册的相应IO事件分发到对应的请求处理器中，进行相应的业务处理。
2. Reactor模式是一种为处理并发服务请求，并将请求提交到一个或多个服务处理程序的事件设计模式。
3. 当客户端请求抵达后，服务处理程序使用<font color='#BAOC2F'>多路分配策略</font>，由一个非阻塞的线程来接收所有请求，然后将请求派发到相关的工作线程并进行处理的过程。
4. 对于高并发系统经常会使用到Reactor模式，用来替代常用的多线程处理方式以节省系统资源并提高系统的吞吐量。

Reactor模型有3个重要组件：

- 多路复用器：由操作系统提供，在linux上一般是select、poll、epoll等系统调用。
- 事件分发器：将多路复用器中返回的就绪事件分到对应的处理函数中。
- 事件处理器：负责处理特定事件的处理函数

![image-20251117154552355](assets/image-20251117154552355.png)

具体流程如下：

1. 注册读就绪事件和相应的事件处理器
2. 事件分发器等待事件。
3. 事件到来激活分发器，分发器调用事件对应的处理器。
4. 事件处理器完成实际的读操作，处理读到的数据

### 事件驱动的网络库设计

reactor模式在项目中的使用：

在server/src/base/netlib目录下有三个头文件及其实现类，BaseSocket.h/.cpp、EventDispatch.h/.cpp、netlib.h/.cpp

这三个文件构成了本即时通信系统中的最核心的网络通信部分，这三个文件详细作用如下：

- CBaseSocket：管理socket io，无论作为client或者server都需要实例化一个CBaseSocket
- CEventDispatch：是reactor的触发器，epoll相关的函数都在此调用
- netlib：是对外提供了调用的api ，它封装了CEventDispatch

```
CBaseSocket：
套接字操作类，保存套接字状态和操作的方法、回调函数（读写关闭回调），listen和connect时都设置回调，
等事件分发器检测有事件的时候，调用相关的回调函数
CEventDispatch：
事件分发器类，封装处理事件循环，处理io操作，和检测定时任务，处理事件回调（调用CBaseSocket中的响应函数）
CImConn：
封装对套接字连接的操作，有自己的接收发送缓冲区，可以设置回调
CImPdu:
消息操作类，封装有关protobuf消息结构的操作方法
netlib:
封装有关socket操作的函数,提供外部调用的接口
CSimpleBuffer:
缓冲区类，封装了对自定义缓冲区的操作方法
CByteStream:
字节流类，封装了对simpleBuffer的操作方法
```

#### CBaseSocket

通过tcp长连接进行网络通信基类：

CBaseSocket中拥有句柄，还有事件处理器（注册的回调函数 m_callback）

1. 每个socket连接都会绑定对应的CImConn，
2. 在读取到一个完整的Pdu后，调用对应的业务方法CLoginConn会继承CImConn，然后去实现具体的HandlePdu业务处理方法，根据业务的处理逻辑
3. 管理socket io，作为client或者server都需要实例化一个CBaseSocket，
4. `unordered_map<net_handle_t, CBaseSocket*> SocketMap;`管理所有的连接，以fd为key，CBaseSocket对象为value，
    - 如果有数据新连接创建，accept得到一个fd，
    - 创建一个对象绑定fd，因为之后需要根据这个fd找到对应的业务封装（需要调用`AddBaseSocket`方法创建CBaseSocket对象），
    - 即net_handle_t对应了新连接fd，而CBaseSocket*对应了一个CBaseSocket对象

CBaseSocket对socket操作进⾏了一些封装，

基于非阻塞套接字的包装类 CBaseSocket 用于在Windows、Linux和MacOS X平台上进行网络通信

1. 封装了套接字的基本操作，使网络编程更加方便和简化
2. 支持监听和接受连接请求，可以用于创建服务器端套接字
3. 支持发起连接到远程服务器，可以用于创建客户端套接字
4. 提供了发送和接收数据的方法，用于在网络上进行数据的传输
5. 处理套接字的事件，例如可读事件、可写事件和关闭事件，通过回调函数的方式进行处理
6. 提供了一些辅助函数，如设置套接字的选项和获取错误代码等

socket定义状态：

```cpp
enum
{
	SOCKET_STATE_IDLE, 			//CBaseSocket()
	SOCKET_STATE_LISTENING, 	//Listen() 作为server的时候用
	SOCKET_STATE_CONNECTING, 	//Connect() 作为client的时候用
	SOCKET_STATE_CONNECTED, 	//OnWrite() 作为client的时候用
	SOCKET_STATE_CLOSING 		//OnClose()
};
```

```cpp
class CBaseSocket : public CRefObject
{
public:
	CBaseSocket();
	virtual ~CBaseSocket();

	SOCKET GetSocket() { return m_socket; }
	void SetSocket(SOCKET fd) { m_socket = fd; }
	void SetState(uint8_t state) { m_state = state; }

	void SetCallback(callback_t callback) { m_callback = callback; }
	void SetCallbackData(void* data) { m_callback_data = data; }
	void SetRemoteIP(char* ip) { m_remote_ip = ip; }
	void SetRemotePort(uint16_t port) { m_remote_port = port; }
	void SetSendBufSize(uint32_t send_size);
	void SetRecvBufSize(uint32_t recv_size);

	const char*	GetRemoteIP() { return m_remote_ip.c_str(); }
	uint16_t	GetRemotePort() { return m_remote_port; }
	const char*	GetLocalIP() { return m_local_ip.c_str(); }
	uint16_t	GetLocalPort() { return m_local_port; }

public:
	// 监听连接/服务器
	int Listen(
		const char*		server_ip, 
		uint16_t		port,
		callback_t		callback,
		void*			callback_data);
	
	// 发起连接/客户端
	net_handle_t Connect(
		const char*		server_ip, 
		uint16_t		port,
		callback_t		callback,
		void*			callback_data);

	int Send(void* buf, int len);
	int Recv(void* buf, int len);
	int Close();
	
	void OnRead();
	void OnWrite();
	void OnClose();

private:
	int _GetErrorCode();
	bool _IsBlock(int error_code);

	void _SetNonblock(SOCKET fd);
	void _SetReuseAddr(SOCKET fd);
	void _SetNoDelay(SOCKET fd);
	void _SetAddr(const char* ip, const uint16_t port, sockaddr_in* pAddr);
	
	void _AcceptNewSocket();
private:
	std::string		m_remote_ip;		//远程IP地址
	uint16_t		m_remote_port;		//远程端口号
	std::string		m_local_ip;			//本地IP地址
	uint16_t		m_local_port;		//本地端口号

	callback_t		m_callback;			//回调函数(用于处理套接字事件)
	void*			m_callback_data;	//回调函数参数

	uint8_t			m_state;			//套接字的状态
	SOCKET			m_socket;			//套接字描述符
};
CBaseSocket* FindBaseSocket(net_handle_t fd);
```

#### CEventDispatch

CEventDispatch事件分发器和Reactor，该类中调用select或epoll循环等待事件发生并处理，添加和删除事件

CEventDispatch：是<font color='#BAOC2F'>reactor的触发器</font>，epoll相关的函数都在此进行调用，网络数据收发

CEventDispatch类 用于实现事件调度器的功能：

该类定义提供了一种机制，可以方便地使用事件调度器来处理套接字事件和定时任务，以 实现高效的事件驱动编程模型

1. 事件调度器（CEventDispatch）用于管理和调度事件和定时器
2. 可以添加套接字的读、写和异常事件，并通过回调函数通知上层逻辑
3. 此外还支持添加定时器和循环任务，以便在指定的时间间隔内执行相应的操作

<mark>CEventDispatch中的函数分类解析</mark>：

- Timer相关：在连接断开，需要重连的情况下需要定时器的机制
    - AddTimer：加入定时事件
    - RemoveTimer：删除定时事件
    - _CheckTimer：检测定时事件（\_私有函数）
- Loop相关：
    - AddLoop：加入循环事件
    - _CheckLoop：检测循环事件，设置回调函数专门处理数据回发（IO线程负责数据的解析、收发，线程池来处理具体的业务）
- epoll相关：对epoll的相关操作
    - AddEvent： 添加fd事件
    - RemoveEvent：删除fd事件
    - StartDispatch：进入reactor主循环
    - StopDispatch：停止reactor主循环

```cpp
#ifndef __EVENT_DISPATCH_H__
#define __EVENT_DISPATCH_H__

#include "ostype.h"
#include "util.h"

#include "Lock.h"

//用于事件类型的常量
enum {
	SOCKET_READ		= 0x1,
	SOCKET_WRITE	= 0x2,
	SOCKET_EXCEP	= 0x4,
	SOCKET_ALL		= 0x7
};


/**
 * CEventDispatch类 用于实现事件调度器的功能
 *  - 事件调度器（CEventDispatch）用于管理和调度事件和定时器
 *  - 可以添加套接字的读、写和异常事件，并通过回调函数通知上层逻辑
 *  - 此外还支持添加定时器和循环任务，以便在指定的时间间隔内执行相应的操作
 * 该类定义提供了一种机制，可以方便地使用事件调度器来处理套接字事件和定时任务，以 实现高效的事件驱动编程模型
*/
class CEventDispatch {
public:
	virtual ~CEventDispatch();

	//添加套接字事件
	void AddEvent(SOCKET fd, uint8_t socket_event);
	//删除套接字事件
	void RemoveEvent(SOCKET fd, uint8_t socket_event);
	//添加定时器
	void AddTimer(callback_t callback, void* user_data, uint64_t interval);
	//删除定时器
	void RemoveTimer(callback_t callback, void* user_data);
	//添加循环任务
    void AddLoop(callback_t callback, void* user_data);

	//启动事件调度器
	void StartDispatch(uint32_t wait_timeout = 100);
	//停止事件调度器
    void StopDispatch();
    //判断事件调度器是否正在运行
    bool isRunning() {return running;}

	//获取事件调度器实例
	static CEventDispatch* Instance();
protected:
	CEventDispatch();

private:
	//检查定时器
	void _CheckTimer();
	//检查循环任务
    void _CheckLoop();

	//定时器结构体 用于存储定时器的信息
	typedef struct {
		callback_t	callback;//回调函数
		void*		user_data;//用户数据
		uint64_t	interval;//时间间隔
		uint64_t	next_tick;//下一次触发的时间戳
	} TimerItem;

private:

#ifdef _WIN32
	//在Windows平台下，定义了三个 fd_set 类型的成员变量
	fd_set	m_read_set;//存储需要监视读事件的套接字集合
	fd_set	m_write_set;//存储需要监视写事件的套接字集合
	fd_set	m_excep_set;//存储需要监视异常事件的套接字集合
#elif __APPLE__
	//在苹果macOS平台定义了一个int类型的成员变量 m_kqfd
	int 	m_kqfd;//用于存储内核事件队列的文件描述符。
#else
	//在其他平台
	int		m_epfd;//用于存储epoll实例的文件描述符
#endif

	CLock			m_lock;//用于保护并发访问的互斥锁
	list<TimerItem*>	m_timer_list;//存储定时器项的链表
	list<TimerItem*>	m_loop_list;//存储循环任务项的链表

	static CEventDispatch* m_pEventDispatch;//静态指针，指向CEventDispatch类的唯一实例
    
    bool running;//事件调度器是否正在运行
};

#endif
```

#### netlib

netlib：是对外提供了调用的api ，它封装了CEventDispatch，主要用于处理tcp连接，实现了一个网络库

<mark>netlib函数分类说明</mark>：

1. netlib_init：初始化网络连接，Linux系统⽆操作，返回NETLIB_OK；
2. netlib_destroy：清除⽹络连接，Linux系统⽆操作，返回NETLIB_OK；

fd相关：

1. netlib_listen：监听连接，底层实现：CBaseSocket返回int
2. netlib_connect：连接，返回net_handle_t
3. netlib_send：发送
4. netlib_recv：接收
5. netlib_close：关闭
6. netlib_option：参数设置
7. <font color='#BAOC2F'>NETLIB_OPT_SET_CALLBACK 设置回调函数</font>（核心）

定时器相关：

1. netlib_register_timer：注册定时器
2. netlib_delete_timer：删除定时器

循环相关：

1. netlib_add_loop：可以加入需要循环处理的事务到reactor
2. netlib_eventloop：进入reactor主循环
3. netlib_stop_event：停⽌事件
4. netlib_is_running：判断是否运⾏

```cpp
//初始化网络库
int netlib_init();
//销毁网络库
int netlib_destroy();

//监听指定的服务器 IP 地址和端口
int netlib_listen(
		const char*	server_ip, 
		uint16_t	port,
		callback_t	callback,
		void*		callback_data);
//连接到指定的服务器 IP 地址和端口
net_handle_t netlib_connect(
		const char*	server_ip,
		uint16_t	port,
		callback_t	callback,
		void*		callback_data);

//发送数据到指定的网络句柄
int netlib_send(net_handle_t handle, void* buf, int len);

//从指定的网络句柄接收数据
int netlib_recv(net_handle_t handle, void* buf, int len);

//关闭指定的网络句柄
int netlib_close(net_handle_t handle);

//设置或获取网络句柄的选项
int netlib_option(net_handle_t handle, int opt, void* optval);

//注册定时器回调函数
int netlib_register_timer(callback_t callback, void* user_data, uint64_t interval);

//删除定时器回调函数
int netlib_delete_timer(callback_t callback, void* user_data);

//添加循环回调函数
int netlib_add_loop(callback_t callback, void* user_data);

//启动网络事件循环，处理网络事件和定时器事件
void netlib_eventloop(uint32_t wait_timeout = 100);

//停止网络事件循环
void netlib_stop_event();

//检查网络事件循环是否在运行
bool netlib_is_running();
```

#### CImConn

事件处理器为CImConn，通过注册imconn_callback到CBaseSocket中，进行回调事件处理器，

而具体事件处理器，客户端和服务端有些不太一样，

- 服务端：事件处理器为继承CImConn的类，重写相关业务逻辑函数。
- 客户端：CImConn中增加ITcpSocketCallback*成员变量，实际是指向TcpClientModule_Impl的指针，主要是为了与客户端做一个组件化操作

网络库数据收发关系：从高层到底层是这样，只举发送数据调用层级 `ImCore::send()->CImConn::send()->netlib_send()->CSocket::send()->(::send)` 中，`netlib` 和 `ImCore` 都封装了一些提供访问和操作网络库的接口，客户端是通过ImCore来进行调用，服务端则是在继承ImCore后直接使用netlib的接口进行数据的发送接收。

#### CSimpleBuffer

之前分析了网络库的数据收发流程，但客户端的如何保证正确读取与发送没有详细讲，这需要依赖<font color='#BAOC2F'>应用层的缓冲区</font>来进行实现，也即CImConn中的CImConn中的m_in_buf和m_out_buf，以下主要针对缓冲区进行分析：

##### 1.缓冲区类型

```cpp
class DLL_MODIFIER CSimpleBuffer
{
public:
    CSimpleBuffer();
    ~CSimpleBuffer();
    uchar_t*  GetBuffer() { return m_buffer; }
    uint32_t GetAllocSize() { return m_alloc_size; }
    uint32_t GetWriteOffset() { return m_write_offset; }
    void IncWriteOffset(uint32_t len) { m_write_offset += len; }
    void Extend(uint32_t len);                    //将缓冲区的大小追加len +len/4
    uint32_t Write(void* buf, uint32_t len);//写入缓冲区
    uint32_t Read(void* buf, uint32_t len);//从缓冲区读出数据
private:
    uchar_t*    m_buffer;//缓冲区地址
    uint32_t    m_alloc_size;//缓冲区容量大小
    uint32_t    m_write_offset;//缓冲区已写入字节大小
};
```

m_in_buf和m_out_buf的类型都是CSimpleBuffer，这个类型很简单，主要使用三个成员变量来保存缓冲区数据信息

```cpp
void CSimpleBuffer::Extend(uint32_t len)
{
    m_alloc_size = m_write_offset + len;
    m_alloc_size += m_alloc_size >> 2;    // increase by 1/4 allocate size
    uchar_t* new_buf = (uchar_t*)realloc(m_buffer, m_alloc_size);
    if(new_buf != NULL)
    {
        m_buffer = new_buf;
    }
}
```

CSimpleBuffer::Extend将缓冲区追加len+len/4大小

```cpp
uint32_t CSimpleBuffer::Write(void* buf, uint32_t len)
{
    if (m_write_offset + len > m_alloc_size) 
    {
        Extend(len);//空间不够进行扩展
    }
    if (buf)
    {
        memcpy(m_buffer + m_write_offset, buf, len);
    }
    m_write_offset += len;
    return len;
}
```

每次写入缓冲区数据时，都需要判断下缓冲区是否足够大小，不够则进行扩展，并将m_write_offset 重新设置为缓冲区现有数据大小

```cpp
uint32_t CSimpleBuffer::Read(void* buf, uint32_t len)
{
    if (len > m_write_offset)
        len = m_write_offset;
    if (buf)
        memcpy(buf, m_buffer, len);
    m_write_offset -= len; //有可能并不是一次性读完
    memmove(m_buffer, m_buffer + len, m_write_offset);
    return len;
}
```

从缓冲区中读出数据，将m_write_offset进行减去读出的数据

本来客户端缓冲区这个东西应该和服务端一起讲的，大部分处理都类似，

而拿出来单独分析的目的是两边分发器采用的监听事件模式有些不一样，

- 客户端采用LT(水平触发模式，只要socket缓冲区中有数据或可写就会一直反复进行通知)，
- 而ET(边沿触发只进行通知一次)，这样就造成了客户端和服务端在应用层上所做的操作就会不一样，
- 下面看看客户端对于数据到达，和发送数据完毕是如何操作的



##### 2.处理数据达到

输入缓冲区，所说的输入，主要针对应用层，对于网络层来说它其实是输出缓冲区，teamtalk这样命名比较符合用户习惯

```cpp
void CImConn::OnRead()
{
    for (;;)//1、保证socket缓冲区数据读完
    {
        uint32_t free_buf_len = m_in_buf.GetAllocSize() - m_in_buf.GetWriteOffset();
        if (free_buf_len < READ_BUF_SIZE)//2、保证缓冲区有足够空闲空间进行存放数据
            m_in_buf.Extend(READ_BUF_SIZE);
        int ret = netlib_recv(m_handle, m_in_buf.GetBuffer() + m_in_buf.GetWriteOffset(), READ_BUF_SIZE);
        if (ret <= 0)
            break;
        m_in_buf.IncWriteOffset(ret);
        //3、读出的数据不足最小包长继续读取数据
        while (m_in_buf.GetWriteOffset() >= imcore::HEADER_LENGTH)
        {
            uint32_t len = m_in_buf.GetWriteOffset();
            uint32_t length = CByteStream::ReadUint32(m_in_buf.GetBuffer());
            if (length > len)//4、如果不足一个包的长度，继续读取数据
                break;
            try
            {
                //5、先解出包头，再解出包体进行业务逻辑处理
                imcore::TTPBHeader pbHeader;
                pbHeader.unSerialize((byte*)m_in_buf.GetBuffer(), imcore::HEADER_LENGTH);
                LOG__(NET, _T("OnRead moduleId:0x%x,commandId:0x%x"), pbHeader.getModuleId(), pbHeader.getCommandId());
                if (m_pTcpSocketCB)
                    m_pTcpSocketCB->onReceiveData((const char*)m_in_buf.GetBuffer(), length);
                LOGBIN_F__(SOCK, "OnRead", m_in_buf.GetBuffer(), length);
            }
            catch (std::exception& ex)
            {
                assert(FALSE);
                LOGA__(NET, "std::exception,info:%s", ex.what());
                if (m_pTcpSocketCB)
                    m_pTcpSocketCB->onReceiveError();
            }
            catch (...)
            {
                assert(FALSE);
                LOG__(NET, _T("unknown exception"));
                if (m_pTcpSocketCB)
                    m_pTcpSocketCB->onReceiveError();
            }
            //6、将输入缓冲区已读的数据清空
            m_in_buf.Read(NULL, length);
        }
    }
}
```

1. `for(;;)`：它的作用是，下面的netlib_recv一次没有将一个包的数据读完，可以进行多次读取，如果此时消息只到达一部分，这个循环会在下一次netlib_recv返回0的时候break，等待下一次数据的读取
2. `if (free_buf_len < READ_BUF_SIZE)` ：如果缓冲区的数据不足128k的时候，就进行扩展，保证每次循环读取数据时，缓冲区中有足够空间
3. `while (m_in_buf.GetWriteOffset() >= imcore::HEADER_LENGTH)` ：这个while在这理其实没有真的进行循环，只是为了既能进行判断，在第四处，又能break掉，实际只会执行一次
4. `if (length > len)` ：这里这样做可以保证读出一个完整的包
5. 先解出包头，再解出包体进行业务逻辑处理：这个地方就是获取到完整包之后的处理，除了消息头付含有一些包长信息，其他的一些处理都是业务范围
6. `m_in_buf.Read(NULL, length);`：在这个函数里面调用memmove，将已从输入缓冲区中读出的数据清空
7. 避免粘包：上面几处的组合，确保一个消息包达到的多种情况都可以完整的读取出来，避免粘包，如：消息包多次到达，
   - 1可以保证数据包能多次从socket中读出，
   - 3保证这个消息包达到一定长度才继续下面的操作 
   - 4保证业务逻辑收到一个完整的包。
   - 还有其他情况如多个消息在不同时间到达或者一起到达



##### 3.消息发送完毕

```cpp
int CImConn::Send(void* data, int len)
{
    if (m_busy)//1、输出缓冲区中有数据
    {
        m_out_buf.Write(data, len);
        return len;
    }
    int offset = 0;
    int remain = len;
    while (remain > 0) {
        int send_size = remain;
        if (send_size > NETLIB_MAX_SOCKET_BUF_SIZE) {
            send_size = NETLIB_MAX_SOCKET_BUF_SIZE;
        }
        int ret = netlib_send(m_handle, (char*)data + offset, send_size);
        if (ret <= 0) {
            ret = 0;
            break;//2、数据发送完毕或者socket缓冲区已满，数据不能继续发送出去
        }
        offset += ret;
        remain -= ret;
    }
    if (remain > 0)
    {
        m_out_buf.Write((char*)data + offset, remain);
        m_busy = true;
        LOG__(NET,  _T("send busy, remain=%d"), m_out_buf.GetWriteOffset());
    }
    return len;
}
```

1. if (m_busy)：在1处，m_busy标识输出缓冲区中是否有数据没有发送出去，如果有，则不能将后续消息包的数据直接发送出去，而是要追加到输出缓冲区末尾，以避免有一个包的数据插在另一包的数据中间，会造成乱序
2. break：这个地方break有两种情况，一种是数据通过while循环全部发送出去，这时remain == 0，数据正常发送完毕(这个完毕也只是数据从应用层到socket缓冲区，数据并不是真的已经发送到网络上了，当然这个不需要我们关心，系统会自动处理)，第二种是如对端接受数据过慢，socket缓冲区不能再容纳更多的数据，remain此时>0，进入下面的if分支，将剩余数据写入输出缓冲区进行存储，等待有可写事件的发生时，再将缓冲区的数据发送出去，下面看看客户端如何进行将剩余数据发送出去的
3. 将剩余数据从缓冲区发送出去：数据发送不出去的时候看看CImConn::Send中的netlib_send做了什么事

```cpp
int netlib_send(net_handle_t handle, void* buf, int len)
{
    CBaseSocket* pSocket = FindBaseSocket(handle);
    if (!pSocket)
    {
        return NETLIB_ERROR;
    }
    int ret = pSocket->Send(buf, len);//继续看这个地方
    pSocket->ReleaseRef();
    return ret;
}

int CBaseSocket::Send(void* buf, int len)
{
    if (m_state != SOCKET_STATE_CONNECTED)
        return NETLIB_ERROR;
    int ret = send(m_socket, (char*)buf, len, 0);    
    if (ret == SOCKET_ERROR)
    {
        int err_code = _GetErrorCode();
        if (_IsBlock(err_code))
        {
#if ((defined _MSC_VER) || (defined __APPLE__))
            CEventDispatch::Instance()->AddEvent(m_socket, SOCKET_WRITE);//这是window的客户端，数据发送不出去会进入这里
#endif
            ret = 0;
        }
        else
        {
            LOG__(NET,  _T("!!!send failed, error code: %d"), err_code);
        }
    }
    else
    {
        LOGBIN_F__(SOCK, "Send", buf, len);
    }
    return ret;
}
```

如上，数据发送失败会调用CEventDispatch::Instance()->AddEvent(m_socket, SOCKET_WRITE);往分发器中为这个socket添加写事件，

如果这个socket可写时会调用pSocket->OnWrite()，如下

```cpp
void CBaseSocket::OnWrite()
{
#if ((defined _MSC_VER) || (defined __APPLE__))
    CEventDispatch::Instance()->RemoveEvent(m_socket, SOCKET_WRITE);//将写事件去除
#endif
    if (m_state == SOCKET_STATE_CONNECTING)
    {
        int error = 0;
        socklen_t len = sizeof(error);
#ifdef _MSC_VER
        getsockopt(m_socket, SOL_SOCKET, SO_ERROR, (char*)&error, &len);
#else
        getsockopt(m_socket, SOL_SOCKET, SO_ERROR, (void*)&error, &len);
#endif
        if (error) {
            m_callback(m_callback_data, NETLIB_MSG_CLOSE, (net_handle_t)m_socket, NULL);
        } else {
            m_state = SOCKET_STATE_CONNECTED;
            m_callback(m_callback_data, NETLIB_MSG_CONFIRM, (net_handle_t)m_socket, NULL);
        }
    }
    else
    {
        m_callback(m_callback_data, NETLIB_MSG_WRITE, (net_handle_t)m_socket, NULL);
    }
}
```

如上，该socket可写后就将socket的写事件移除，因为客户端采用LT模式，不移除会不断触发写事件，造成busy loop，

下面的回调在之前的网络框架分析中讲了是调用imconn_callback，根据第二个参数来调用imconn的不同反应函数，这里是调用pConn->OnWrite()

```cpp
void CImConn::OnWrite()
{
    if (!m_busy)
        return;
    while (m_out_buf.GetWriteOffset() > 0) {
        int send_size = m_out_buf.GetWriteOffset();
        if (send_size > NETLIB_MAX_SOCKET_BUF_SIZE) {
            send_size = NETLIB_MAX_SOCKET_BUF_SIZE;
        }
        int ret = netlib_send(m_handle, m_out_buf.GetBuffer(), send_size);
        if (ret <= 0) {
            ret = 0;
            break;
        }
        m_out_buf.Read(NULL, ret);//发送多少数据，就将缓冲区清楚多少数据
    }
    if (m_out_buf.GetWriteOffset() == 0) {
        m_busy = false;
    }
    LOG__(NET, _T("onWrite, remain=%d"), m_out_buf.GetWriteOffset());
}
```

这段代码就比较容易了，循环发送数据，直至数据发送完毕，然后将m_busy标识为没有数据，

如果发送了一部分，则在netlib_send里继续添加写事件，等待下一次可以发送的时候进行发送数据，至此，数据发送完毕这个操作就讲完了



### 数据包处理流程

#### 1.公共事件分发器

```cpp
void CEventDispatch::StartDispatch(uint32_t wait_timeout)
{
    fd_set read_set, write_set, excep_set;
    timeval timeout;
    timeout.tv_sec = 1;    //wait_timeout 1 second
    timeout.tv_usec = 0;
    while (running)
    {
        //_CheckTimer();
        //_CheckLoop();
        if (!m_read_set.fd_count && !m_write_set.fd_count && !m_excep_set.fd_count)
        {
            Sleep(MIN_TIMER_DURATION);
            continue;
        }
        m_lock.lock();
        FD_ZERO(&read_set);
        FD_ZERO(&write_set);
        FD_ZERO(&excep_set);
        memcpy(&read_set, &m_read_set, sizeof(fd_set));
        memcpy(&write_set, &m_write_set, sizeof(fd_set));
        memcpy(&excep_set, &m_excep_set, sizeof(fd_set));
        m_lock.unlock();
        if (!running)
            break;
        //for (int i = 0; i < read_set.fd_count; i++) {
        //    LOG__(NET,  "read fd: %d\n", read_set.fd_array[i]);
        //}
        int nfds = select(0, &read_set, &write_set, &excep_set, &timeout);
        if (nfds == SOCKET_ERROR)
        {
            //LOG__(NET,  "select failed, error code: %d\n", GetLastError());
            Sleep(MIN_TIMER_DURATION);
            continue;            // select again
        }
        if (nfds == 0)
        {
            continue;
        }
        for (u_int i = 0; i < read_set.fd_count; i++)
        {
            //LOG__(NET,  "select return read count=%d\n", read_set.fd_count);
            SOCKET fd = read_set.fd_array[i];
            CBaseSocket* pSocket = FindBaseSocket((net_handle_t)fd);
            if (pSocket)
            {
                pSocket->OnRead();
                pSocket->ReleaseRef();
            }
        }
        for (u_int i = 0; i < write_set.fd_count; i++)
        {
            //LOG__(NET,  "select return write count=%d\n", write_set.fd_count);
            SOCKET fd = write_set.fd_array[i];
            CBaseSocket* pSocket = FindBaseSocket((net_handle_t)fd);
            if (pSocket)
            {
                pSocket->OnWrite();
                pSocket->ReleaseRef();
            }
        }
        for (u_int i = 0; i < excep_set.fd_count; i++)
        {
            LOG__(NET,  _T("select return exception count=%d"), excep_set.fd_count);
            SOCKET fd = excep_set.fd_array[i];
            CBaseSocket* pSocket = FindBaseSocket((net_handle_t)fd);
            if (pSocket)
            {
                pSocket->OnClose();
                pSocket->ReleaseRef();
            }
        }
    }
}
```

#### 2.句柄所对应的事件处理器

句柄所对应的事件处理器pSocket->OnRead()，

上述分发器代码是window下的，linux下的类似，在这个函数中select等待事件的发生，有事件则调用对应句柄所对应的事件处理器，如pSocket->OnRead()

```cpp
void CBaseSocket::OnRead()
{
    if (m_state == SOCKET_STATE_LISTENING)
    {
        //这个服务端才有，如果socket句柄是lisent时使用的句柄，检测到客户端有新连接过来会调用
        _AcceptNewSocket();
    }
    else
    {
        u_long avail = 0;
        if ( (ioctlsocket(m_socket, FIONREAD, &avail) == SOCKET_ERROR) || (avail == 0) )
        {
            //m_callback保存imconn.h中的imconn_callback地址
            m_callback(m_callback_data, NETLIB_MSG_CLOSE, (net_handle_t)m_socket, NULL);
        }
        else
        {
            m_callback(m_callback_data, NETLIB_MSG_READ, (net_handle_t)m_socket, NULL);
        }
    }
}
```

#### 3.新连接接收

`_AcceptNewSocket();`

在客户端有新链接来时会调用，增加新的会话链接CImConn，并将回调函数 `imconn_callback` 保存 `CBaseSocket` 的m_callback中，

如果该链接是非监听状态，则调用之前注册的回调imconn_callback

```cpp
void imconn_callback(void* callback_data, uint8_t msg, uint32_t handle, void* pParam)
{
    NOTUSED_ARG(handle);
    NOTUSED_ARG(pParam);
    CImConn* pConn = TcpSocketsManager::getInstance()->get_client_conn(handle);
    if (!pConn)
    {
        //LOG__(NET, _T("connection is invalied:%d"), handle);
        return;
    }
    pConn->AddRef();
    //    LOG__(NET,  "msg=%d, handle=%d\n", msg, handle);
    switch (msg)
    {
    case NETLIB_MSG_CONFIRM:
        pConn->onConnect();
        break;
    case NETLIB_MSG_READ:
        pConn->OnRead();
        break;
    case NETLIB_MSG_WRITE:
        pConn->OnWrite();
        break;
    case NETLIB_MSG_CLOSE:
        pConn->OnClose();
        break;
    default:
        LOG__(NET,  _T("!!!imconn_callback error msg: %d"), msg);
        break;
    }
    pConn->ReleaseRef();
}
```

#### 4.事件处理

事件处理pConn->OnRead()，

上述代码为windows下的，linux下服务端代码类似，这里举有数据到来举例，有数据的过来的时候调用这个回调，通过选择相应的分支 pConn->OnRead();

```cpp
void CImConn::OnRead()
{
    for (;;)
    {
        uint32_t free_buf_len = m_in_buf.GetAllocSize() - m_in_buf.GetWriteOffset();
        if (free_buf_len < READ_BUF_SIZE)
            m_in_buf.Extend(READ_BUF_SIZE);
        int ret = netlib_recv(m_handle, m_in_buf.GetBuffer() + m_in_buf.GetWriteOffset(), READ_BUF_SIZE);
        if (ret <= 0)
            break;
        m_in_buf.IncWriteOffset(ret);
        while (m_in_buf.GetWriteOffset() >= imcore::HEADER_LENGTH)
        {
            uint32_t len = m_in_buf.GetWriteOffset();
            uint32_t length = CByteStream::ReadUint32(m_in_buf.GetBuffer());
            if (length > len)
                break;
            try
            {
                imcore::TTPBHeader pbHeader;
                pbHeader.unSerialize((byte*)m_in_buf.GetBuffer(), imcore::HEADER_LENGTH);
                LOG__(NET, _T("OnRead moduleId:0x%x,commandId:0x%x"), pbHeader.getModuleId(), pbHeader.getCommandId());
                if (m_pTcpSocketCB)
                    //在这个进行相应的业务逻辑处理
                    m_pTcpSocketCB->onReceiveData((const char*)m_in_buf.GetBuffer(), length);
                LOGBIN_F__(SOCK, "OnRead", m_in_buf.GetBuffer(), length);
            }
            catch (std::exception& ex)
            {
                assert(FALSE);
                LOGA__(NET, "std::exception,info:%s", ex.what());
                if (m_pTcpSocketCB)
                    m_pTcpSocketCB->onReceiveError();
            }
            catch (...)
            {
                assert(FALSE);
                LOG__(NET, _T("unknown exception"));
                if (m_pTcpSocketCB)
                    m_pTcpSocketCB->onReceiveError();
            }
            m_in_buf.Read(NULL, length);
        }
    }
}
```

#### 5.OnRead函数

客户端和服务端的CImConn::OnRead()流程类似，先解包，然后再进行调用业务逻辑处理函数，

只不过客户端是，m_pTcpSocketCB->onReceiveData，服务端是重写CImConn::HandlePdu()

- 客户端：

```cpp
void TcpClientModule_Impl::onReceiveData(const char* data, int32_t size)
{
    if (m_pServerPingTimer)
        m_pServerPingTimer->m_bHasReceivedPing = TRUE;
    imcore::TTPBHeader header;
    header.unSerialize((byte*)data, imcore::HEADER_LENGTH);    
    if (IM::BaseDefine::CID_OTHER_HEARTBEAT == header.getCommandId() && IM::BaseDefine::SID_OTHER == header.getModuleId())
    {
        //模块器端过来的心跳包，不跳到业务层派发
        return;
    }
    LOG__(NET, _T("receiveData message moduleId:0x%x,commandId:0x%x")
        , header.getModuleId(), header.getCommandId());
    if (g_seqNum == header.getSeqNumber())
    {
        m_pImLoginResp->ParseFromArray(data + imcore::HEADER_LENGTH, size - imcore::HEADER_LENGTH);
        ::SetEvent(m_eventReceived);
        return;
    }
    //将网络包包装成任务放到逻辑任务队列里面去
    _handlePacketOperation(data, size);
}

//m_pTcpSocketCB->onReceiveData客户端一般数据是TcpClientModule_Impl::onReceiveData这收取，文件传输数据使用的是文件传输模块的onReceiveData，暂且不提，在这个函数中，获取数据包中的serviceID和cmdID，根据不同serviceID选择相应模块的实例

void TcpClientModule_Impl::_handlePacketOperation(const char* data, UInt32 size)
{
    std::string copyInBuffer(data, size);
    imcore::IMLibCoreStartOperationWithLambda(
        [=]()
    {
        imcore::TTPBHeader header;
        header.unSerialize((byte*)copyInBuffer.data(),imcore::HEADER_LENGTH);
        module::IPduPacketParse* pModule
            = (module::IPduPacketParse*)__getModule(header.getModuleId());
        if (!pModule)
        {
            assert(FALSE);
            LOG__(ERR, _T("module is null, moduleId:%d,commandId:%d")
                , header.getModuleId(), header.getCommandId());
            return;
        }
        std::string pbBody(copyInBuffer.data() + imcore::HEADER_LENGTH, size - imcore::HEADER_LENGTH);
        pModule->onPacket(header, pbBody);
    });
}

//不同模块都重写了onPacket函数，根据cmdID的不同来选择相应的业务处理函数，如
void GroupListModule_Impl::onPacket(imcore::TTPBHeader& header, std::string& pbBody)
{
    switch (header.getCommandId())
    {
    case IM::BaseDefine::CID_GROUP_NORMAL_LIST_RESPONSE :
        _groupNormalListResponse(pbBody);
        break;
    case IM::BaseDefine::CID_GROUP_INFO_RESPONSE:
        _groupInfoResponse(pbBody);
        break;
    case IM::BaseDefine::CID_GROUP_CREATE_RESPONSE:
        _groupCreateDiscussGroupResponse(pbBody);
        break;
    case IM::BaseDefine::CID_GROUP_CHANGE_MEMBER_RESPONSE:
        _groupChangedGroupMembersResponse(pbBody);
        break;
    case IM::BaseDefine::CID_GROUP_SHIELD_GROUP_RESPONSE:
        _groupShieldResponse(pbBody);
        break;
    case IM::BaseDefine::CID_GROUP_CHANGE_MEMBER_NOTIFY:
        _groupChangeMemberNotify(pbBody);
        break;
    default:
        break;
    }
}
```

- 服务端：回到上面服务端重写CImConn::HandlePdu()流程，例如

```cpp
void CLoginConn::HandlePdu(CImPdu* pPdu)
{
 switch (pPdu->GetCommandId()) {
        case CID_OTHER_HEARTBEAT:
            break;
        case CID_OTHER_MSG_SERV_INFO:
            _HandleMsgServInfo(pPdu);
            break;
        case CID_OTHER_USER_CNT_UPDATE:
            _HandleUserCntUpdate(pPdu);
            break;
        case CID_LOGIN_REQ_MSGSERVER:
            _HandleMsgServRequest(pPdu);
            break;
        default:
            log("wrong msg, cmd id=%d ", pPdu->GetCommandId());
            break;
     }
}
```



### 数据包收包发包流程

#### 1.客户端收发包

**发包**

1. TcpClientModule_Impl::sendPacket()->_sendPacket()组包头和包体，包头有对应的serviceID和cmid还有包的总长度，包体是真正的业务结构数据，序列化后发送
2. 调用IMLibCoreWrite()将组好的包给CImConn发送
3. CImConn::Send()发送不出去就在自己的缓存中保存，监听写事件，可写时将数据发送出去，就用netlib_send()
4. netlib_send()找出对应socket的CBaseSocket，调用CBaseSocket::Send()发送数据
5. CBaseSocket::Send()则调用相应系统底层的send()函数，进行数据发送

**收包**

1. 事件分发器监听事件的发生，有读事件发生，根据socket句柄，调用拥有该句柄的CBaseSocket的OnRead()函数
2. CBaseSocket::OnRead()有数据调用回调函数imconn_callback()（函数中的_AcceptNewSocket上面已做说明，不再解释）
3. imconn_callback()中判断是读是写数据，调用CImConn::OnRead()
4. CImConn::OnRead()中调用onReceiveData()进行整个包的数据的读取，并调用TcpClientModule_Impl::_handlePacketOperation()
5. _handlePacketOperation（）中封装一个操作函数到lambda操作类中放入逻辑处理队列（逻辑队列的概念后续章节再讲，目前先清楚这个lambda表达式会被调用即可）
6. lambda表达式中根据header.getModuleId调用相应的模块，并将包体从包中解出，将包头和包体都传给相应模块的业务函数onPacket()
7. onPacket在各个模块中根据命令号，来进行相应的业务处理

#### 2.服务端收发包

举消息服务器回应心跳的例子，都其他服务器都差不多

**发包**

1. CMsgConn::_HandleHeartBeat(CImPdu *pPdu)中直接将客户端发过来的心跳数据发回去，调用父类CImConn::SendPdu()
2. CImConn::SendPdu()调用CimConn::Send()，后面发送的流程就跟客户端一样
3. CImConn::Send()发送不出去就在自己的缓存中保存，监听写事件，可写时将数据发送出去，就用netlib_send()
4. netlib_send()找出对应socket的CBaseSocket，调用CBaseSocket::Send()发送数据
5. CBaseSocket::Send()则调用相应系统底层的send()函数，进行数据发送

**收包**

服务端收包前三步跟客户端一样，直接复制

1. 事件分发器监听事件的发生，有读事件发生，根据socket句柄，调用拥有该句柄的CBaseSocket的OnRead()函数
2. CBaseSocket::OnRead()有数据调用回调函数imconn_callback()（函数中的_AcceptNewSocket上面已做说明，不再解释）
3. imconn_callback()中判断是读是写数据，调用CImConn::OnRead()
4. CImConn::OnRead()根据调用子类中重写的HandlePdu（）函数，然后根据各自业务逻辑选择相应函数

本章主要针对客户端和服务端共同的网络库，进行分析，

还有些缓冲区等也想放在这个章节分析，但两端采用的是不同的事件触发模式，客户端采用select水平触发模式(LT)，服务端采用epoll的边沿触发模式(ET)，所以就放到后面分别单独讲述



### 网络库应用-作为客户端

reactor模型在客户端与服务器都可以使用，作为msg_server作为客户端去连接login_server服务器，msg_server如何与login_server保持通信？

![image-20230518172656465](assets/image-20230518172656465.png)

#### 1.netlib

![image-20230519083205478](assets/image-20230519083205478.png)

windows需要调用`netlib_init()`方法，

一个reactor可以作为服务端管理多个客户端连接的业务，也能作为客户端去连接其他服务器，如login_server等，

回调事件：

```cpp
enum {
  NETLIB_MSG_CONNECT = 1,
  NETLIB_MSG_CONFIRM,
  NETLIB_MSG_READ,
  NETLIB_MSG_WRITE,
  NETLIB_MSG_CLOSE,
  NETLIB_MSG_TIMER,
  NETLIB_MSG_LOOP
};
```

#### 2.BaseSocket

![image-20230519083345717](assets/image-20230519083345717.png)

```cpp
enum {
	SOCKET_STATE_IDLE,
	SOCKET_STATE_LISTENING,
	SOCKET_STATE_CONNECTING,
	SOCKET_STATE_CONNECTED,
	SOCKET_STATE_CLOSING
};
```

1. CBaseSocket刚被初始化时，是处于SOCKET_STATE_IDLE状态，
2. 如果CBaseSocket作为服务器端，则CBaseSocket将会被更新为SOCKET_STATE_LISTENING状态，
3. 如果有调用`OnWrite()`方法状态会更新为SOCKET_STATE_CONNECTING
4. 当真正连接上的时候可以进行数据写入发送数据时（作为客户端），包括有新连接进入时`AcceptNewSocket`（作为服务端），CBaseSocket将会被更新为SOCKET_STATE_CONNECTED状态，
5. 当调用`OnClose()`方法时，CBaseSocket将会被更新为SOCKET_STATE_CLOSING状态，

#### 3.EventDispatch（核心）

![image-20230519120457157](assets/image-20230519120457157.png)

以msg_server为例分析整个流程，msg_server如何连接login_server？

##### msg_server.cpp

```cpp
  char* listen_ip = config_file.GetConfigName("ListenIP");
  char* str_listen_port = config_file.GetConfigName("ListenPort");
  char* ip_addr1 = config_file.GetConfigName("IpAddr1");  // 电信IP
  char* ip_addr2 = config_file.GetConfigName("IpAddr2");  // 网通IP
  char* str_max_conn_cnt = config_file.GetConfigName("MaxConnCnt");
  char* str_aes_key = config_file.GetConfigName("aesKey");
  uint32_t db_server_count = 0;
  serv_info_t* db_server_list = read_server_config(&config_file, "DBServerIP", "DBServerPort", db_server_count);

  uint32_t login_server_count = 0;
  serv_info_t* login_server_list = read_server_config(&config_file, "LoginServerIP", "LoginServerPort", login_server_count);

  uint32_t route_server_count = 0;
  serv_info_t* route_server_list = read_server_config(&config_file, "RouteServerIP", "RouteServerPort", route_server_count);

  uint32_t push_server_count = 0;
  serv_info_t* push_server_list = read_server_config(&config_file, "PushServerIP", "PushServerPort", push_server_count);

  uint32_t file_server_count = 0;
  serv_info_t* file_server_list = read_server_config(&config_file, "FileServerIP", "FileServerPort", file_server_count);
```

```cpp
  printf("server start listen on: %s:%d\n", listen_ip, listen_port);

  init_msg_conn();

  init_file_serv_conn(file_server_list, file_server_count);

  init_db_serv_conn(db_server_list2, db_server_count2, concurrent_db_conn_cnt);

  init_login_serv_conn(login_server_list, login_server_count, ip_addr1, ip_addr2, listen_port, max_conn_cnt);

  init_route_serv_conn(route_server_list, route_server_count);

  init_push_serv_conn(push_server_list, push_server_count);
  printf("now enter the event loop...\n");
```

##### LoginServerConn.cpp:CImConn

```cpp
/* LoginServerConn.h */
#ifndef LOGINSERVCONN_H_
#define LOGINSERVCONN_H_

#include "ServInfo.h"
#include "imconn.h"

class CLoginServConn : public CImConn {
 public:
  CLoginServConn();
  virtual ~CLoginServConn();

  bool IsOpen() { return m_bOpen; }

  void Connect(const char* server_ip, uint16_t server_port, uint32_t serv_idx);
  virtual void Close();

  virtual void OnConfirm();
  virtual void OnClose();
  virtual void OnTimer(uint64_t curr_tick);23w3we
  virtual void HandlePdu(CImPdu* pPdu);

 private:
  bool m_bOpen;
  uint32_t m_serv_idx;
};

void init_login_serv_conn(serv_info_t* server_list, uint32_t server_count,
                          const char* msg_server_ip_addr1,
                          const char* msg_server_ip_addr2,
                          uint16_t msg_server_port, uint32_t max_conn_cnt);
bool is_login_server_available();
void send_to_all_login_server(CImPdu* pPdu);

#endif /* MSGCONN_LS_H_ */
```

```cpp
/* LoginServerConn.cpp部分代码 */
void login_server_conn_timer_callback(void* callback_data, uint8_t msg, uint32_t handle, void* pParam) {
  ConnMap_t::iterator it_old;
  CLoginServConn* pConn = NULL;
  uint64_t cur_time = get_tick_count();

  for (ConnMap_t::iterator it = g_login_server_conn_map.begin(); it != g_login_server_conn_map.end();) {
    //如果已经连接 则会触发对应的业务
	it_old = it;
    it++;

	//每个连接都有自己的业务 
    pConn = (CLoginServConn*)it_old->second;
    pConn->OnTimer(cur_time);
  }

  // reconnect LoginServer 检测是否需要重新连接
  serv_check_reconnect<CLoginServConn>(g_login_server_list, g_login_server_count);
}

void init_login_serv_conn(serv_info_t* server_list, uint32_t server_count,
                          const char* msg_server_ip_addr1,
                          const char* msg_server_ip_addr2,
                          uint16_t msg_server_port, uint32_t max_conn_cnt) {
  g_login_server_list = server_list;
  g_login_server_count = server_count;

  serv_init<CLoginServConn>(g_login_server_list, g_login_server_count);

  g_msg_server_ip_addr1 = msg_server_ip_addr1;
  g_msg_server_ip_addr2 = msg_server_ip_addr2;
  g_msg_server_port = msg_server_port;
  g_max_conn_cnt = max_conn_cnt;

  //定时器 每隔1秒触发定时器 检测与login_server是否有连接 如果没有连接则重新进行去连接
  netlib_register_timer(login_server_conn_timer_callback, NULL, 1000);
}
```

##### ServInfo.h

```cpp
template <class T>
void serv_check_reconnect(serv_info_t* server_list, uint32_t server_count) {
  T* pConn;
  for (uint32_t i = 0; i < server_count; i++) {
    pConn = (T*)server_list[i].serv_conn;
    if (!pConn) {
      server_list[i].idle_cnt++;
      if (server_list[i].idle_cnt >= server_list[i].reconnect_cnt) {
        pConn = new T();
        pConn->Connect(server_list[i].server_ip.c_str(), server_list[i].server_port, i);
        server_list[i].serv_conn = pConn;
      }
    }
  }
}
```

```cpp
/* LoginServerConn::Connect */
void CLoginServConn::Connect(const char* server_ip, uint16_t server_port, uint32_t serv_idx) {
  log("Connecting to LoginServer %s:%d ", server_ip, server_port);
  //作为服务器 关注数据可读 作为客户端连接别人 关注数据可写
  m_serv_idx = serv_idx;
  m_handle = netlib_connect(server_ip, server_port, imconn_callback, (void*)&g_login_server_conn_map);
  
  if (m_handle != NETLIB_INVALID_HANDLE) {
    g_login_server_conn_map.insert(make_pair(m_handle, this));
  }
}
```

##### imconn.cpp

```cpp
/* imconn.h */
#ifndef IMCONN_H_
#define IMCONN_H_

#include "ImPduBase.h"
#include "netlib.h"
#include "util.h"

#define SERVER_HEARTBEAT_INTERVAL 5000
#define SERVER_TIMEOUT 30000
#define CLIENT_HEARTBEAT_INTERVAL 30000
#define CLIENT_TIMEOUT 120000
#define MOBILE_CLIENT_TIMEOUT 60000 * 5
#define READ_BUF_SIZE 2048

class CImConn : public CRefObject {
 public:
  CImConn();
  virtual ~CImConn();

  bool IsBusy() { return m_busy; }
  int SendPdu(CImPdu* pPdu) {
    return Send(pPdu->GetBuffer(), pPdu->GetLength());
  }
  int Send(void* data, int len);

  virtual void OnConnect(net_handle_t handle) { m_handle = handle; }
  virtual void OnConfirm() {}
  virtual void OnRead();
  virtual void OnWrite();
  virtual void OnClose() {}
  virtual void OnTimer(uint64_t curr_tick) {}
  virtual void OnWriteCompelete(){};

  virtual void HandlePdu(CImPdu* pPdu) {}

 protected:
  net_handle_t m_handle;
  bool m_busy;

  string m_peer_ip;
  uint16_t m_peer_port;
  CSimpleBuffer m_in_buf;  // socket读取数据缓存
  CSimpleBuffer m_out_buf;

  bool m_policy_conn;
  uint32_t m_recv_bytes;
  uint64_t m_last_send_tick;
  uint64_t m_last_recv_tick;
  uint64_t m_last_all_user_tick;
};

typedef hash_map<net_handle_t, CImConn*> ConnMap_t;
typedef hash_map<uint32_t, CImConn*> UserMap_t;

void imconn_callback(void* callback_data, uint8_t msg, uint32_t handle,
                     void* pParam);
void ReadPolicyFile();

#endif /* IMCONN_H_ */
```

每一个业务相关的连接都需要去继承CImConn类，

imconn.cpp中对应者事件分发，

```cpp
/* imconn.cpp部分代码 */
void imconn_callback(void* callback_data, uint8_t msg, uint32_t handle, void* pParam) {
  NOTUSED_ARG(handle);
  NOTUSED_ARG(pParam);

  if (!callback_data) return;

  ConnMap_t* conn_map = (ConnMap_t*)callback_data;
  CImConn* pConn = FindImConn(conn_map, handle);
  if (!pConn) return;

  // log("msg=%d, handle=%d ", msg, handle);
  switch (msg) {
    case NETLIB_MSG_CONFIRM:
      pConn->OnConfirm();
      break;
    case NETLIB_MSG_READ:
      pConn->OnRead();
      break;
    case NETLIB_MSG_WRITE:
      pConn->OnWrite();
      break;
    case NETLIB_MSG_CLOSE:
      pConn->OnClose();
      break;
    default:
      log_error("!!!imconn_callback error msg: %d ", msg);
      break;
  }

  pConn->ReleaseRef();
}
```

##### LoginServerConn.cpp:CImConn::OnConfirm

```cpp
void CLoginServConn::OnConfirm() {
  log("connect to login server success ");
  m_bOpen = true;
  g_login_server_list[m_serv_idx].reconnect_cnt = MIN_RECONNECT_CNT / 2;

  uint32_t cur_conn_cnt = 0;
  uint32_t shop_user_cnt = 0;

  list<user_conn_t> user_conn_list;
  CImUserManager::GetInstance()->GetUserConnCnt(&user_conn_list, cur_conn_cnt);
  char hostname[256] = {0};
  gethostname(hostname, 256);
  IM::Server::IMMsgServInfo msg;
  msg.set_ip1(g_msg_server_ip_addr1);
  msg.set_ip2(g_msg_server_ip_addr2);
  msg.set_port(g_msg_server_port);
  msg.set_max_conn_cnt(g_max_conn_cnt);
  msg.set_cur_conn_cnt(cur_conn_cnt);
  msg.set_host_name(hostname);
  CImPdu pdu;
  pdu.SetPBMsg(&msg);
  pdu.SetServiceId(SID_OTHER);
  pdu.SetCommandId(CID_OTHER_MSG_SERV_INFO);
  SendPdu(&pdu);
}
```

#### 4.函数调用逻辑

![image-20230519133330335](assets/image-20230519133330335.png)

![image-20230519134332129](assets/image-20230519134332129.png)

msg_server访问login_server的流程：

1. 开启1000ms定时器，定时检测login_server之间的连接关系
    - 如果没有连接 则进行重连
    - 如果已经连接则定时向login_server发送心跳包
2. 有用户上线或者下线时向msg_server上报目前其的负载情况



### 网络库应用-作为服务端

login_server 作为服务端，如何响应客户端的连接并处理？客户端 -> login_server http请求

![image-20230519135918782](assets/image-20230519135918782.png)

#### 1.http_callback

```cpp
  /* login_server.cpp */
  CStrExplode http_listen_ip_list(http_listen_ip, ';');
  for (uint32_t i = 0; i < http_listen_ip_list.GetItemCnt(); i++) {
    ret = netlib_listen(http_listen_ip_list.GetItem(i), http_port, http_callback, NULL);
    if (ret == NETLIB_ERROR) return ret;
  }
```

```cpp
/* login_server.cpp */
// Android、IOS、PC等客户端请求连接事件
void http_callback(void* callback_data, uint8_t msg, uint32_t handle, void* pParam) {
  if (msg == NETLIB_MSG_CONNECT) {
    // 这里是不是觉得很奇怪,为什么new了对象却没有释放?
    // 实际上对象在被Close时使用delete this的方式释放自己
    CHttpConn* pConn = new CHttpConn();
    pConn->OnConnect(handle);
  } else {
    log_error("!!!error msg: %d ", msg);
  }
}
```

```cpp
/* HttpConn.cpp */
void CHttpConn::OnConnect(net_handle_t handle) {
  log("OnConnect, handle=%d\n", handle);
  m_sock_handle = handle;
  m_state = CONN_STATE_CONNECTED;
  g_http_conn_map.insert(make_pair(m_conn_handle, this));

  netlib_option(handle, NETLIB_OPT_SET_CALLBACK, (void*)httpconn_callback);
  netlib_option(handle, NETLIB_OPT_SET_CALLBACK_DATA, reinterpret_cast<void*>(m_conn_handle));
  netlib_option(handle, NETLIB_OPT_GET_REMOTE_IP, (void*)&m_peer_ip);
}
```



#### 2.msg_serv_callback

```cpp
  /* login_server.cpp */
  CStrExplode msg_server_listen_ip_list(msg_server_listen_ip, ';');
  for (uint32_t i = 0; i < msg_server_listen_ip_list.GetItemCnt(); i++) {
    ret = netlib_listen(msg_server_listen_ip_list.GetItem(i), msg_server_port, msg_serv_callback, NULL);
    if (ret == NETLIB_ERROR) return ret;
  }
```

```cpp
/* login_server.cpp */
// this callback will be replaced by imconn_callback() in OnConnect()
// msg_server请求连接事件
void msg_serv_callback(void* callback_data, uint8_t msg, uint32_t handle, void* pParam) {
  log("msg_server come in");

  if (msg == NETLIB_MSG_CONNECT) {
    CLoginConn* pConn = new CLoginConn();
    pConn->OnConnect2(handle, LOGIN_CONN_TYPE_MSG_SERV);
  } else {
    log_error("!!!error msg: %d ", msg);
  }
}
```

```cpp
/* LoginConn.cpp */
void CLoginConn::OnConnect2(net_handle_t handle, int conn_type) {
  m_handle = handle;
  m_conn_type = conn_type;
  ConnMap_t* conn_map = &g_msg_serv_conn_map;
  if (conn_type == LOGIN_CONN_TYPE_CLIENT) {
    conn_map = &g_client_conn_map;
  } else {
    conn_map->insert(make_pair(handle, this));
  }
  netlib_option(handle, NETLIB_OPT_SET_CALLBACK, (void*)imconn_callback);
  netlib_option(handle, NETLIB_OPT_SET_CALLBACK_DATA, (void*)conn_map);
}
```

### 相关问题

#### epoll如何在teamtalk中发回作用？

在TeamTalk中，epoll是一种在Linux系统上使用的I/O事件通知机制，用于处理并发连接和事件。

它在TeamTalk服务器中的作用主要体现在以下几个方面：

1. 高并发连接管理：TeamTalk服务器需要处理大量的并发连接，包括客户端的连接请求和数据传输。使用epoll可以实现高效的并发连接管理，通过注册和监听连接事件，服务器可以同时处理多个连接而不需要为每个连接创建独立的线程，提高服务器的性能和可扩展性。
2. 异步事件处理：通过将连接的I/O事件注册到epoll中，服务器可以异步地监听和处理事件，而不需要阻塞在每个连接上。当有数据到达、连接关闭或其他事件发生时，epoll会通知服务器，服务器可以相应地进行数据读取、处理和发送等操作，实现高效的事件驱动模型。
3. 提高资源利用率：epoll使用事件驱动的方式，仅在有事件发生时才会通知服务器，避免了频繁的轮询和资源浪费。它能够有效利用系统资源，提高CPU和内存的利用率，并且在连接数较多时表现较好。
4. 定时器管理：epoll还可以结合定时器机制，用于管理连接的超时和心跳检测。通过在epoll中注册定时器事件，服务器可以定期检查连接的状态，判断是否超时或需要发送心跳包，从而实现连接的管理和维护。

总的来说，epoll在TeamTalk中发挥了关键的作用，通过<font color='#BAOC2F'>高效的并发连接管理和异步事件处理</font>，提高了服务器的性能和可扩展性。它允许服务器同时处理多个连接和事件，并且能够有效利用系统资源，实现高效的事件驱动模型。同时，结合定时器机制，还能管理连接的超时和心跳检测，保证连接的稳定性和可靠性。



#### 即时通讯引入reactor模式

即时通讯系统通常需要处理大量的并发连接和事件，以实现实时消息传递和实时状态更新。在这种情况下，使用Reactor模式是一种常见的设计模式，有以下几个原因：

1. 高并发处理：Reactor模式允许即时通讯服务器同时处理多个并发连接和事件。它使用事件驱动的方式，将每个连接和事件注册到一个中心事件循环（即Reactor），然后由Reactor根据事件的发生情况调用相应的处理程序。
2. 高性能：Reactor模式在处理大量并发连接时具有较高的性能。通过使用事件驱动的方式，它避免了每个连接都需要一个独立的线程来处理的开销，而是利用事件循环在一个或少数几个线程中高效处理多个连接。
3. 可扩展性：Reactor模式支持服务器的可扩展性。由于每个连接和事件都是异步处理的，因此可以轻松地增加或减少服务器的处理能力。通过增加更多的Reactor线程或使用多核处理器，可以提高系统的吞吐量和并发连接数。
4. 简化编程模型：Reactor模式提供了一种清晰、结构化的编程模型。通过将每个事件的处理逻辑封装到相应的事件处理程序中，可以更好地组织和维护代码。同时，它还提供了一些方便的工具和库，如事件分发器、定时器等，简化了开发人员的工作。

Reactor模式是一种适用于高并发、高性能、可扩展的即时通讯系统的设计模式，它能够有效地处理大量的并发连接和事件，并提供简化的编程模型。

在许多即时通讯系统中，客户端与服务器之间通常需要建立和维持TCP长连接。这种长连接的使用有以下几个好处：

1. 实时消息传递：通过建立TCP长连接，客户端和服务器可以实时地进行消息传递。客户端可以即时发送消息给服务器，服务器也可以即时将消息推送给客户端，实现实时的双向通信。
2. 减少连接建立和断开的开销：TCP的连接建立和断开是有开销的，包括网络延迟和资源消耗。通过使用长连接，客户端和服务器之间的连接可以一直保持，避免了频繁的连接建立和断开操作，减少了相关开销。
3. 减少服务器负载：在短连接的情况下，每次客户端需要与服务器进行通信时都需要建立一个新的连接。当有大量客户端同时连接服务器时，这将给服务器带来较大的负载压力。而使用长连接可以减少连接数，降低服务器的负载。
4. 心跳检测和保活机制：通过长连接，客户端和服务器可以定期发送心跳消息来检测连接的健康状态。如果检测到连接异常断开，可以及时采取恢复或重新连接的措施，保证连接的稳定性和可靠性。

客户端与服务端之间建立TCP长连接可以实现实时消息传递、减少连接建立和断开的开销，降低服务器负载，并提供心跳检测和保活机制，从而提供稳定、可靠的通信环境。

#### 扩展并发量的方法？

扩展TeamTalk的并发量可以通过以下几种方式实现：

1. 使用多线程/多进程：通过在服务器端使用多线程或多进程的方式，可以同时处理多个客户端连接和请求。每个线程/进程负责处理一部分连接和请求，从而提高并发处理能力。这种方式需要考虑线程/进程间的同步和通信机制，以及负载均衡的问题。
2. 使用线程池：使用线程池可以更好地管理和控制并发连接。线程池中的线程可以复用，避免了线程的频繁创建和销毁开销，提高了性能和效率。通过合理设置线程池的大小和调度策略，可以适应不同负载情况和并发需求。
3. 使用异步I/O模型：采用异步I/O模型（如epoll、IOCP等）可以提高并发处理能力。异步I/O模型使用事件驱动的方式，通过异步操作和回调函数处理连接和数据，避免了阻塞和线程消耗，提高了系统的吞吐量和并发能力。
4. 分布式架构：将系统分布到多个物理或虚拟服务器上，通过负载均衡和分布式调度，将并发连接和请求均匀地分配到不同的服务器上进行处理。这样可以将负载分散到多个节点上，提高系统的整体并发能力。
5. 优化算法和数据结构：对于特定的业务场景，可以通过优化算法和数据结构来提高并发处理能力。例如，使用高效的哈希算法、索引结构或缓存技术，减少数据库或磁盘的访问频率，加快数据查询和处理速度。













































